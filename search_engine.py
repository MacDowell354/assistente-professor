import os
from llama_index.core import (
    SimpleDirectoryReader,
    GPTVectorStoreIndex,
    StorageContext,
    load_index_from_storage,
    Settings,
)
from llama_index.embeddings.openai import OpenAIEmbedding

# ğŸ“ DiretÃ³rio e caminho do Ã­ndice
INDEX_DIR = "storage"
INDEX_FILE = os.path.join(INDEX_DIR, "index.json")

# ğŸ”‘ Configura a API Key da OpenAI
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    raise ValueError("âŒ OPENAI_API_KEY nÃ£o encontrada nas variÃ¡veis de ambiente.")

# ğŸ¤– Define o modelo de embedding
Settings.embed_model = OpenAIEmbedding(
    model="text-embedding-3-small",
    api_key=api_key,
)

def load_or_build_index():
    """Carrega o Ã­ndice existente ou cria um novo a partir de transcricoes.txt."""
    if os.path.exists(INDEX_FILE):
        print("ğŸ“ Ãndice encontrado. Carregando do disco...")
        storage_context = StorageContext.from_defaults(persist_dir=INDEX_DIR)
        return load_index_from_storage(storage_context)
    else:
        print("âš™ï¸ Ãndice nÃ£o encontrado. Construindo novo...")
        # carrega apenas o transcricoes.txt (preservando maiÃºsculas e formataÃ§Ã£o)
        docs = SimpleDirectoryReader(input_files=["transcricoes.txt"]).load_data()
        index = GPTVectorStoreIndex.from_documents(docs)
        # persiste para usos futuros
        index.storage_context.persist(persist_dir=INDEX_DIR)
        print(f"âœ… Ãndice construÃ­do com {len(docs)} documentos.")
        return index

# âš¡ Inicializa o Ã­ndice na importaÃ§Ã£o deste mÃ³dulo
index = load_or_build_index()

def retrieve_relevant_context(
    question: str,
    top_k: int = 3,
    chunk_size: int = 512
) -> str:
    """
    Busca no Ã­ndice atÃ© `top_k` trechos que respondam Ã  `question`.
    Usa `chunk_size` para controlar o tamanho dos blocos de texto.
    Retorna string vazia se nÃ£o encontrar algo relevante.
    """
    print("ğŸ” DEBUG â€” Pergunta para contexto:", question)

    # cria um engine de consulta mais flexÃ­vel
    engine = index.as_query_engine(
        similarity_top_k=top_k,
        chunk_size=chunk_size
    )

    response = engine.query(question)
    response_str = str(response).strip()
    print("ğŸ” DEBUG â€” Contexto bruto retornado:", response_str)

    # normaliza para checagens
    lower = response_str.lower()
    if not lower or lower in ("none", "null"):
        print("ğŸ” DEBUG â€” Contexto vazio apÃ³s normalizaÃ§Ã£o")
        return ""

    # evita respostas genÃ©ricas ou pedidos de desculpa
    frases_bloqueadas = ["nÃ£o tenho certeza", "desculpe", "nÃ£o sei"]
    if any(frase in lower for frase in frases_bloqueadas):
        print("ğŸ” DEBUG â€” Contexto bloqueado por frase de incerteza")
        return ""

    # filtra menÃ§Ãµes a tÃ³picos fora do escopo (vÃ­deo, Instagram etc.)
    termos_proibidos = [
        "instagram", "vÃ­deos para instagram", "celular para gravar", "smartphone",
        "tiktok", "post viral", "gravar vÃ­deos", "microfone", "cÃ¢mera",
        "ediÃ§Ã£o de vÃ­deo", "hashtags", "stories", "marketing de conteÃºdo",
        "produÃ§Ã£o de vÃ­deo", "influencer"
    ]
    if any(tp in lower for tp in termos_proibidos):
        print("ğŸ” DEBUG â€” Contexto bloqueado por termo proibido")
        return ""

    # passa adiante o trecho completo com formataÃ§Ã£o original
    print("ğŸ” DEBUG â€” Contexto final aceito:", response_str)
    return response_str
