import os
from llama_index.core import (
    SimpleDirectoryReader,
    GPTVectorStoreIndex,
    StorageContext,
    load_index_from_storage,
    Settings,
)
from llama_index.embeddings.openai import OpenAIEmbedding

INDEX_DIR = "storage"
INDEX_FILE = os.path.join(INDEX_DIR, "index.json")

# Carrega a API key do ambiente
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    raise ValueError("âŒ OPENAI_API_KEY nÃ£o encontrada nas variÃ¡veis de ambiente.")

# Define o embedding model
embed_model = OpenAIEmbedding(
    model="text-embedding-3-small",
    api_key=api_key,
)
Settings.embed_model = embed_model

# Carrega ou cria o Ã­ndice
def load_or_build_index():
    if os.path.exists(INDEX_FILE):
        print("ğŸ“ Ãndice encontrado. Carregando...")
        storage_context = StorageContext.from_defaults(persist_dir=INDEX_DIR)
        return load_index_from_storage(storage_context)
    else:
        print("ğŸ› ï¸ Ãndice nÃ£o encontrado. Construindo novo...")
        docs = SimpleDirectoryReader(input_files=["transcricoes.txt"]).load_data()
        index = GPTVectorStoreIndex.from_documents(docs)
        index.storage_context.persist(persist_dir=INDEX_DIR)
        return index

# Inicializa o Ã­ndice
index = load_or_build_index()

# FunÃ§Ã£o para buscar contexto relevante da pergunta
def retrieve_relevant_context(question: str, top_k: int = 3) -> str:
    engine = index.as_query_engine(similarity_top_k=top_k)
    response = engine.query(question)
    response_str = str(response).strip()

    # âœ… Verifica se a resposta estÃ¡ vazia ou genÃ©rica
    if not response_str or response_str.lower() in ["", "none", "null"]:
        return ""

    # ğŸ”’ Restringe retornos genÃ©ricos irrelevantes
    if "nÃ£o tenho certeza" in response_str.lower() or "desculpe" in response_str.lower():
        return ""

    return response_str
